services:
  llm-kg-api:
    build:
      context: .
      dockerfile: llm_kg/Dockerfile
    container_name: llm-kg-api
    environment:
      LLM_PROVIDER: "${LLM_PROVIDER:-ollama}"
      OPENAI_BASE_URL: "${OPENAI_BASE_URL:-http://host.docker.internal:8080/v1}"
      OPENAI_API_KEY: "${OPENAI_API_KEY:-}"
      GEMINI_BASE_URL: "${GEMINI_BASE_URL:-https://generativelanguage.googleapis.com/v1beta}"
      GEMINI_API_KEY: "${GEMINI_API_KEY:-}"
      GEMINI_MODEL: "${GEMINI_MODEL:-gemini-3-pro-preview}"
      GEMINI_INPUT_TOKEN_LIMIT: "${GEMINI_INPUT_TOKEN_LIMIT:-1048576}"
      GEMINI_OUTPUT_TOKEN_LIMIT: "${GEMINI_OUTPUT_TOKEN_LIMIT:-65536}"
      OLLAMA_BASE_URL: "${OLLAMA_BASE_URL:-http://host.docker.internal:11434}"
      OLLAMA_MODEL: "${OLLAMA_MODEL:-llama3.2:latest}"
      OLLAMA_THINK: "${OLLAMA_THINK:-true}"
      OLLAMA_THINK_JSON: "${OLLAMA_THINK_JSON:-false}"
      LLM_MODEL: "${LLM_MODEL:-mlx-community/Qwen3-8B-4bit-DWQ-053125}"
      EXTRACTION_MODEL: "${EXTRACTION_MODEL:-sam860/deepseek-r1-0528-qwen3:8b}"
      NL2CYPHER_MODEL: "${NL2CYPHER_MODEL:-ministral-3:14b}"
      LLM_TIMEOUT_SECONDS: "${LLM_TIMEOUT_SECONDS:-180}"
      LLM_TEMPERATURE: "${LLM_TEMPERATURE:-0.1}"
      LLM_MAX_TOKENS: "${LLM_MAX_TOKENS:-2048}"
      CHUNK_SIZE_MODE: "${CHUNK_SIZE_MODE:-provider}"
      CHUNK_SIZE_TOKENS: "${CHUNK_SIZE_TOKENS:-1048576}"
      CHUNK_MIN_TOKENS: "${CHUNK_MIN_TOKENS:-120}"
    ports:
      - "18000:8000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health', timeout=2)"]
      interval: 10s
      timeout: 3s
      retries: 5
      start_period: 8s

  neo4j:
    image: neo4j:5-community
    container_name: llm-kg-neo4j
    environment:
      NEO4J_AUTH: neo4j/password
    ports:
      - "7474:7474"
      - "7687:7687"
    volumes:
      - neo4j_data:/data

volumes:
  neo4j_data:
